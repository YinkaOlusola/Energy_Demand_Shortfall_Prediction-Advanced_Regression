{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3635fa11",
   "metadata": {},
   "source": [
    "# Spain Electricity Shortfall Challenge\n",
    "\n",
    "\n",
    "## Regression Project Student Solution\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "The government of Spain is considering an expansion of it's renewable energy resource infrastructure investments. As such, they require information on the trends and patterns of the countries renewable sources and fossil fuel energy generation. Your company has been awarded the contract to:\n",
    "\n",
    "- 1. analyse the supplied data;\n",
    "- 2. identify potential errors in the data and clean the existing data set;\n",
    "- 3. determine if additional features can be added to enrich the data set;\n",
    "- 4. build a model that is capable of forecasting the three hourly demand shortfalls;\n",
    "- 5. evaluate the accuracy of the best machine learning model;\n",
    "- 6. determine what features were most important in the model’s prediction decision, and\n",
    "- 7. explain the inner working of the model to a non-technical audience.\n",
    "\n",
    "Formally the problem statement was given to you, the senior data scientist, by your manager via email reads as follow:\n",
    "\n",
    "> In this project you are tasked to model the shortfall between the energy generated by means of fossil fuels and various renewable sources - for the country of Spain. The daily shortfall, which will be referred to as the target variable, will be modelled as a function of various city-specific weather features such as `pressure`, `wind speed`, `humidity`, etc. As with all data science projects, the provided features are rarely adequate predictors of the target variable. As such, you are required to perform feature engineering to ensure that you will be able to accurately model Spain's three hourly shortfalls.\n",
    " \n",
    "On top of this, she has provided you with a starter notebook containing vague explanations of what the main outcomes are. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94366c5c",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#zero>I. Problem Statement</a>\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa55fbec",
   "metadata": {},
   "source": [
    " <a id=\"zero\"></a>\n",
    "## I. Problem Statement\n",
    "\n",
    "To prevent the drastic effect of climate change and ensure sustainability of the global ecosystem, the world is gradually adopting the use of renewable energy. Asides ensuring a steady supply of electricity for a productive livelihood, renewable energy sources has also led to the emergence of new energy markets, enterprises, and job opportunities.\n",
    "\n",
    "Renewable energy sources accounted for 43% of all electricity generated in Spain in the year 2020. As a result, the government of Spain is considering an expansion of its renewable energy resource infrastructure investments. To do so, they need information on the country's renewable resource and fossil fuel energy generating trends and patterns.\n",
    "\n",
    "Our team of data scientists have been tasked with creating a model that would help predict the three-hourly load shortfall between the energy generated by means of fossil fuels and various renewable sources in Spain. This information will aid the government in determining how much infrastructure spending should be increased.\n",
    "\n",
    "[Load Shortfall Image](https://dailytimes.com.pk/assets/uploads/2022/04/29/5ef6cf3f8fe3c.jpg)\n",
    "\n",
    "![Loadshedding](https://github.com/JayHansea/TEAM-NM2/blob/65985167bb4b2ce180e3217d9b1b5356c9047a4d/Electricity%20Shortfall%20Image.jpg?raw=true)\n",
    "\n",
    "[Image Source](https://dailytimes.com.pk/927865/pakistanis-suffer-worst-loadshedding-as-electricity-shortfall-reaches-9000mw/)\n",
    "\n",
    "\n",
    "\n",
    "### II. OBJECTIVES\n",
    "\n",
    "* Explore and visualize the dataset.\n",
    "* Clean and engineer the dataset.\n",
    "* Build several models that predicts the 3 hourly load shortfall.\n",
    "* Assess the accuracy of the models.\n",
    "* Choose the best model to make predictions.\n",
    "\n",
    "\n",
    "### III. FEATURES DESCRIPTION\n",
    "* **Time**: The date and time of the day when each feature value was recorded\n",
    "* **Wind_speed**: This is a measure of the wind speed recorded in each city\n",
    "* **Wind_deg**: This is a measure of the direction of the wind in each city\n",
    "* **Pressure**: It is the atmospheric pressure measured in each city\n",
    "* **Rain_1h/Rain_3h**: This is the amount of rain in each city as recorded in hourly or 3 hourly intervals\n",
    "* **Snow**: The amount of snowfall in each city\n",
    "* **Cloud_all**: This is a measure of the percentage of cloud coverage in each city\n",
    "\n",
    "\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997462e2",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "| First we import, and briefly describe the libraries that will be used throughout our analysis and modelling. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec03746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for data loading, data manipulation and data visulisation\n",
    "import numpy as np   \n",
    "import pandas as pd   \n",
    "\n",
    "# Libraries for data preparation and model building\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "\n",
    "#Visualization Packages\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew \n",
    "import seaborn as sns \n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppresing warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a6718",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing & Previewing Data ⚡ |\n",
    "| :--------------------------- |\n",
    "|Here we imported the given datasets (train and test dataset) as sourced from [Kaggle](https://www.kaggle.com/competitions/spain-electricity-shortfall-challenge-2022/data). It is important to note that this dataset was first pushed to our Github repository from where it was then loaded to this notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffee21e",
   "metadata": {},
   "source": [
    "There are two data set provided in this project:\n",
    " - **df_train:** This is the data set that was used for the Exploratory Data Analysis (EDA) and model building and evaluatioin.\n",
    " \n",
    " - **df_test:** This data set was used for Kaggle submission. Data preprocessing and engineering were carried out on it as needed based on the works done on the train dataset and the dataset was fed into the trained model to make predictions for the Kaggle competition. The works on the dataset is covered in the kaggle submission notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811f0486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train data\n",
    "df_train = pd.read_csv('Data/df_train.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "df_test = pd.read_csv('Data/df_test.csv')\n",
    "df_test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81132ab3",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, we performed an in-depth analysis of all the variables in the DataFrame. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d0e407",
   "metadata": {},
   "source": [
    "### Checking the \"shape\" of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8786c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c51e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5571c",
   "metadata": {},
   "source": [
    "First, we checked the shape of both datasets, and observed that the training dataset has 49 columns while the test dataset has only 48 coulmns. The missing column from the test set is the target variable the (load_shortfall_3hr) that our model is to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ded1a4",
   "metadata": {},
   "source": [
    "### Features Preview\n",
    "\n",
    "It is imperative to have an overview of all features in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329e5bcf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = pd.DataFrame(df_train.columns, columns=['Features'])\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd01d3a",
   "metadata": {},
   "source": [
    "There unnamed column can be dropped as it does not represent any feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c84017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the unnamed feature\n",
    "df_train = df_train.drop(['Unnamed: 0'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a79457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612c5a80",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Summary Statistics\n",
    "\n",
    "The describe() function was used to generate descriptive statistics that summarizes the central tendency, dispersion \n",
    "and shape of the dataset, excluding null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25811b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03aea3d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Splitting the dataset by cities\n",
    "\n",
    "In the following cells, we took a deeper dive into the train dataset by splitting it into the different cities. This was done to allow us assess the impact of city specific features in relation to the output variable.\n",
    "\n",
    "In the following cells, a deeper look was taken at the train dataset by splitting it into different cities. This was done to assess the impact of city specific features in relation to the output variable - `load_shortfall_3h`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7028a3e0",
   "metadata": {},
   "source": [
    "**Checking the number of weather conditions recorded in each city and what those weather conditions are.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2318c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of weather conditions recorded for each city\n",
    "\n",
    "num_of_features = {}\n",
    "\n",
    "for col in df_train.columns[1:-1]: # Excluding time and load_shortfall_3h columns\n",
    "    city = col.split('_')[0]\n",
    "    if city in num_of_features.keys():\n",
    "        num_of_features[city] += 1\n",
    "    else:\n",
    "        num_of_features[city] = 1\n",
    "\n",
    "# Viewing the number of weather conditins recorded for various cities in the dataset       \n",
    "num_of_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13de98f1",
   "metadata": {},
   "source": [
    "**Weather condition recorded for each city**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990aa8a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Weather conditions recorded for each city\n",
    "\n",
    "city_weather_records = {}\n",
    "\n",
    "for col in df_train.columns[1:-1]:  # Excluding time and load_shortfall_3h columns\n",
    "    city = col.split('_')[0]\n",
    "    if city in city_weather_records.keys():\n",
    "        city_weather_records[city].append(col)\n",
    "    else:\n",
    "        city_weather_records[city] = [col]\n",
    "\n",
    "# Viewing the number of weather conditins recorded for various cities in the dataset       \n",
    "city_weather_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604dc5c0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Null values\n",
    "Identifying the column(s) with null entries is important so it doesnt affect the performance of the trained model. Using the isnull() function shows the number of null values present in the dataset. The function shows that only the `Valencia_pressure` feature has null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d2ecd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking for null values in the columns\n",
    "\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3f70b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming the existence of null values in the column\n",
    "\n",
    "df_train[df_train['Valencia_pressure'].isnull()]['Valencia_pressure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719c9c32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "sns.boxplot(df_train['Valencia_pressure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e0200c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "From the boxplot above, the `valencia_pressure` feature has values between 1010 and less than 1020\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0606ff23",
   "metadata": {},
   "source": [
    "**Mean, Median and Mode of `Valencia_pressure`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c7dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Summary statistics of Valencia_pressure\n",
    "\n",
    "print('Mean:', df_train['Valencia_pressure'].mean(), '\\n')\n",
    "\n",
    "print('Median:', df_train['Valencia_pressure'].median(), '\\n')\n",
    "\n",
    "print('Mode:', df_train['Valencia_pressure'].mode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2044150",
   "metadata": {},
   "source": [
    "### Choosing the measure to replace null values\n",
    "\n",
    "As observed all three measures of central tendency have very similar values, as such there wouldn't be too much of a difference which measure we choose to go with. However, for the purpose of this model, we shalll replace the null values in \"Valencia pressure\" with the mean, since it is a better measure for the datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f3c3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the null values with the mean\n",
    "\n",
    "df_clean_train = df_train\n",
    "df_clean_train['Valencia_pressure'] = df_clean_train['Valencia_pressure'].fillna(df_clean_train['Valencia_pressure'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09526ca9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confirming that the null values have been replaced\n",
    "\n",
    "df_clean_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb376601",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fca11c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### DATA TYPES\n",
    "\n",
    "Machine learning models only work with numeric data, which means the data types for the models must be floats or integers in order to get the best predictions out of the models built from the dataset. Also, having the right data types is needed for visualizing the dataset during the Exploratory Data Analysis (EDA).The code below reveals the data types of the data contained in data set. Note that the `time` data, the `Valencia_wind_deg` data as well as the `Seville_pressure` are all object data (also known as strings). These have to be converted to floats or integers for them to be usable both for EDA and model training. Similar to the null values, they can either be dropped or converted.The former is not recommended as everytime a data is dropped, potentially valuable information that may be very useful for model building are lost. A more beneficial approach will be to process this data by Transforming it to numeric form or encoding it to a form that the model can utilize. 3 non-numeric objects are observed from the data set. they are `time`, `Valencia_wind_deg` and `Seville_pressure`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5733b1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_clean_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56ca742",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Modifying the time feature\n",
    "\n",
    "The `time` feature is mapped into a datetime format which is the appropriate data type for date and time features and other features can be created from the date time feature that can be used in the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae7503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_train['time'] = pd.to_datetime(df_clean_train['time'])\n",
    "df_clean_train.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1982ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d26c21",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Visualizing the Load shortfall as a function of time\n",
    "\n",
    "This datetime feature can now be used to create some time-dependent plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74581472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the load_shortfall as a function of time\n",
    "\n",
    "sns.set(style=\"whitegrid\") # Style for the plot\n",
    "plt.figure(figsize=(20, 7)) # Setting the figure size\n",
    "\n",
    "sns.lineplot(data=df_clean_train, x='time', y='load_shortfall_3h')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49356d7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The plot above is showing very little information. Visualizing the load shortfall as a function of other date-time parameters may provide better insight to the pattern of the load short fall.\n",
    "\n",
    "As such other date time parameters were extracted from the time feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d5b691",
   "metadata": {},
   "source": [
    "### Extracting other Datetime feature from the time feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9df1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list =[]\n",
    "\n",
    "# second\n",
    "df_clean_train['second'] = df_clean_train['time'].dt.second\n",
    "\n",
    "# minute\n",
    "df_clean_train['minute'] = df_clean_train['time'].dt.minute\n",
    "\n",
    "# hour\n",
    "df_clean_train['hour'] = df_clean_train['time'].dt.hour\n",
    "\n",
    "# day\n",
    "df_clean_train['Day'] = df_clean_train['time'].dt.day\n",
    "\n",
    "# month\n",
    "df_clean_train['Month'] = df_clean_train['time'].dt.month\n",
    "\n",
    "# year\n",
    "df_clean_train['Year'] = df_clean_train['time'].dt.year\n",
    "\n",
    "\n",
    "# adding the new features to the dataset \n",
    "column_list = ['time', 'second', 'minute', 'hour','Day','Month','Year'] + list(df_clean_train.columns[1:-6])\n",
    "\n",
    "\n",
    "\n",
    "df_clean_train = df_clean_train[column_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448694c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24bca43",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Checking the nature of values in the `second` and `minute` columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e242c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of entries in the second and minute columns that are zeros\n",
    "\n",
    "df_clean_train[(df_clean_train['second'] == 0) & (df_clean_train['minute'] == 0)][['second', 'minute']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e537bf4e",
   "metadata": {},
   "source": [
    "\n",
    "From the result above, the values in the `second` and `minute` columns are all zeros and as such will be dropped from the dataframe.\n",
    "\n",
    "It is equally necessary to check the range of values in the remaining date and time columns, that is, `hour`, `day`, `month`, and `year`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc6381f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Dropping the `second` and `minute` columns from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a08903a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean_train.drop(columns=['second', 'minute'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e973bc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Number of unique values in all datetime columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14b0ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique values in the hour column\n",
    "\n",
    "df_clean_train['hour'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87203c42",
   "metadata": {},
   "source": [
    "It's reasonable to see that the hour are in multiples of three as the shortfalls were recorded every three hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af83448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique values in the Day column\n",
    "\n",
    "df_clean_train['Day'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e491ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique values in the Month column\n",
    "\n",
    "df_clean_train['Month'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0ee586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique values in the Year column\n",
    "\n",
    "df_clean_train['Year'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c293e9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Visualizing the Load shortfall with respect to `Hour`, `Day`, and `Month`\n",
    "\n",
    "It is rather intuitive to visualize the `load_shortfall` in `hour`, `Day`, and `Month`.\n",
    "\n",
    "To achieve this, the following were done:\n",
    " - The index of the dataframe used in plotting these visualizations was set to be the time.\n",
    " - The dataframe was resampled based on the time interval being considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861929ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualizing the load_shortfall for every 3 hours intervals\n",
    "\n",
    "print()\n",
    "\n",
    "plt.figure(figsize=(25, 7))\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "plt.plot(df_clean_train.set_index('time').resample('H').ffill()['load_shortfall_3h'])\n",
    "\n",
    "plt.title(\"3-Hourly Load Shortfall\")\n",
    "plt.xlabel(\"Time (3-Hourly)\")\n",
    "plt.ylabel(\"Load Shortfall\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbfd717",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "From the plot above, the shortfall can be seen to show some regular pattern with peaks at specific points and intervals. This visualization can be drilled down to get better intuition into this pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c120b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualizing the load_shortfall for every day\n",
    "\n",
    "print()\n",
    "\n",
    "plt.figure(figsize=(20, 7)) # Setting the figure size\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "#sns.lineplot(data=df_clean_train, x='Day', y='load_shortfall_3h')\n",
    "plt.plot(df_clean_train.set_index('time').resample('D').ffill()['load_shortfall_3h'])\n",
    "\n",
    "plt.title(\"Daily Load Shortfall\")\n",
    "plt.xlabel(\"Day of week\")\n",
    "plt.ylabel(\"Load Shortfall\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3acd9a7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The shortfall shows inherent pattern across days of week and this will be further investigated by lookingat the pattern within months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf289633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the load_shortfall for every month\n",
    "\n",
    "print()\n",
    "\n",
    "plt.figure(figsize=(15, 3)) # Setting the figure size\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "plt.plot(df_clean_train.set_index('time').resample('W').ffill()['load_shortfall_3h'])\n",
    "\n",
    "plt.title(\"Weekly Load Shortfall\")\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"Load Shortfall\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f9463b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e19121",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualizing the load_shortfall for every month\n",
    "\n",
    "print()\n",
    "\n",
    "plt.figure(figsize=(15, 3)) # Setting the figure size\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "plt.plot(df_clean_train.set_index('time').resample('M').ffill()['load_shortfall_3h'])\n",
    "\n",
    "plt.title(\"Load Shortfall versus Month\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Load Shortfall\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eece50",
   "metadata": {},
   "source": [
    "\n",
    "In the visualization above, the shortfall can be seen to follow a trend across the three years interval of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd1a9c5",
   "metadata": {},
   "source": [
    "### Distribution of weather data across cities\n",
    "\n",
    "To get a better intuiton into the nature of the data set and see how different weather records compare across different cities it is imperative to visualize different weather conditions across cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74dddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather conditions recorded for all the cities excluding wind degree and 3-hourly rainfall\n",
    "\n",
    "weather_type_records = {}\n",
    "\n",
    "for col in df_clean_train.columns[5:-1]:  # Excluding time and load_shortfall_3h columns\n",
    "    weather_type = col.split('_')[1]\n",
    "    #if (col.split('_')[-1] not in ['deg', '3h']):\n",
    "    if (weather_type in weather_type_records.keys()):\n",
    "        weather_type_records[weather_type].append(col)\n",
    "    else:\n",
    "        weather_type_records[weather_type] = [col]\n",
    "\n",
    "# Viewing the number of weather conditins recorded for various cities in the dataset       \n",
    "weather_type_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f184e6c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**The Histograms below help in visualizing the nature of the dataset and how they compare among themselves.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2561922",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histograms to show the distribution of weather conditions for different cities.\n",
    "\n",
    "for weather in weather_type_records.keys():\n",
    "    cities_weather = weather_type_records[weather]\n",
    "    weather_name = f'{weather}_df'\n",
    "    weather_name = df_clean_train[cities_weather]\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    sns.set_context(font_scale=.2)\n",
    "    weather_name.hist(layout = (4, 4), figsize=(10, 8))\n",
    "    \n",
    "    plt.tight_layout(pad=3.0)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a036ad31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histograms to show the distribution of weather conditions for different cities.\n",
    "\n",
    "for weather in weather_type_records.keys():\n",
    "    cities_weather = weather_type_records[weather]\n",
    "    weather_name = f'{weather}_df'\n",
    "    weather_name = df_clean_train[cities_weather]\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    weather_name.plot(kind='density', subplots=True, layout=(4,4), sharex=False, figsize=(10,8))\n",
    "    \n",
    "    plt.tight_layout(pad=2.0)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2291c769",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The histograms above shows the distribution of the various weather conditions for the different cities. The following can be observed:\n",
    " - The wind speed values are left skewed with majority of the values between 0 and 5. Some\n",
    " - The wind degree have a different distribution from the wind speed, with a distribution spread around 200\n",
    " - The one-hourly rain measure has has most of its value close to 0 and less than 1 while the 3 hourly rain measure are less than 0.0. This measures will be further explored to decide whether to drop them from the dataframe or not before proceeding to the model training stage.\n",
    " - The three cities with humidity measure have the distributions range less than 100\n",
    " - For the cloud measure, the majority of the distribution are very close to zero except for that of Bilbao\n",
    " - The distribution of the pressure for all cities are between 975 and 1025 with most of the data being above 1000 except for Barcelona which seems to have been recorded using a different unit.\n",
    " - The 3-hourly snow measure both have different range of values and this will be further explored.\n",
    " - All the weather_id of the cities have similar distribution and what this data represents should be explored.\n",
    " - Every of the temperature distribution are similar and the min and max features will be dropped, but that will be delayed till later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4404f178",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Line plots of weather data across cities|**\n",
    "\n",
    "The trend of each weather data recorded across the cities will be explored here excluding wind degree and 3-hourly rainfall that have shown a large divergence from the rest of the data set in the distribution visualization done above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00e9cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather conditions recorded for all the cities excluding wind degree and 3-hourly rainfall\n",
    "\n",
    "weather_type_records_2 = {}\n",
    "\n",
    "for col in df_clean_train.columns[5:-1]:  # Excluding time and load_shortfall_3h columns\n",
    "    weather_type = col.split('_')[1]\n",
    "    if (col.split('_')[-1] not in ['deg']):\n",
    "        if (weather_type in weather_type_records_2.keys()):\n",
    "            weather_type_records_2[weather_type].append(col)\n",
    "        else:\n",
    "            weather_type_records_2[weather_type] = [col]\n",
    "\n",
    "# Viewing the number of weather conditins recorded for various cities in the dataset       \n",
    "#weather_type_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1824454",
   "metadata": {},
   "source": [
    "**Creating the plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b4f428",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Line plots to show the distribution of weather conditions for different cities.\n",
    "\n",
    "for weather in weather_type_records_2.keys():\n",
    "    cities_weather = list(weather_type_records_2[weather])\n",
    "    weather_name = f'{weather}'\n",
    "    cities_weather.insert(0, 'time')\n",
    "    weather_df = df_clean_train[cities_weather]\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    sns.set_context(font_scale=1)\n",
    "    weather_df.set_index('time').resample('M').ffill().plot(layout = (4, 4), figsize=(10, 3))\n",
    "    plt.xlim(np.datetime64('2014-10-01'), np.datetime64('2018-04-01'))\n",
    "    plt.title(weather_name)\n",
    "    \n",
    "    plt.tight_layout(pad=2.0)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1d45d5",
   "metadata": {},
   "source": [
    "The plots above shows how each weather condition varies with time across each cities and as it can be seen, the distributions seem to exhibit some similarities except for some few.\n",
    " - Snow weather records seem to show little to no variation. This feature will be explored further to identify the nature of the data present in them and decide whether to leave or drop them from the dataframe.\n",
    " - Also, the temperature features all show very similar trends and as such there may be high multicollinearity between them. Moreover, the max and min values of these features can be dropped leaving only the actual features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d7b0e3",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Data Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Data engineering ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section we cleaned the dataset, and created new features - as identified in the EDA phase. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02053c9b",
   "metadata": {},
   "source": [
    "During the Exploratory Data Analysis, some data/feature engineering were done, such as investigating and fixing null values, datatypes of entries, and features with only zero entries. These were done to aid the exploratory data analysis that was done.\n",
    "\n",
    "A this stage, further data/feature engineering will be carried out, more imprtantly for the purpose of the machine learning model training.\n",
    "\n",
    "<br>\n",
    "\n",
    "   From the plots of the distribution of weather data across cities that was created during Exploratory Data Analysis, it was pointed out that the one-hourly rain measure has has most of its value close to 0 and less than 1 while the 3 hourly rain measure are less than 0.0. This features call for further investigation and that is what will be done next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f726589c",
   "metadata": {},
   "source": [
    "### Exploring the rain features\n",
    "\n",
    "**Checking for the number of zero entries in the three columns with rain data**\n",
    "\n",
    "The code below can be used to obtain all rain features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb524db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a list of features that represents rain weather condition\n",
    "\n",
    "rain_feature = []\n",
    "\n",
    "for feature in df_clean_train.columns:\n",
    "    if (len(feature.split('_')) > 1):\n",
    "        if (feature.split('_')[1] == 'rain'):\n",
    "            rain_feature.append(feature)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f95ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the number of zero entries in all the rain features\n",
    "\n",
    "df_clean_train[df_clean_train[rain_feature] != 0][rain_feature].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce8267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bee3d12",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Of all the values above, `Bilbao_rain_1h` has the highest number of no-zero values and this represents 29.6 % of its total entries. To be sure the zero entries are not due to approximation, the number of decimal places displayed by the dataframe was increased and a particular feature was checked. Below is what was obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa952a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.8f}'.format)\n",
    "\n",
    "df_clean_train['Bilbao_rain_1h']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f434bb85",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Rather than drop these features, they will be left in the dataframe as it is rather logical to have a few records for rainfal since there cannot be rainfall throughout the year. The high number of zero entries only implies that there are few instances of rainfall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9428640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "838b4f8d",
   "metadata": {},
   "source": [
    "#### Number of Zero entries for each feature of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b445e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_train[df_clean_train[:] == 0][:].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0c7353",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Zero entries in the dataset for some features do not represent missing values but rather, shows the absence of that condition within that time frame of the measurement. Examples are the rain and snow features which contain a lot of zeros. This is logical as zero entries represent the absence of those weather conditions since weather conditions are expected to vary through the year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452e38fa",
   "metadata": {},
   "source": [
    "### Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6123f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd607809",
   "metadata": {},
   "source": [
    "There are two object data types in the dataframe. This two features will be investigated further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce000b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_train['Valencia_wind_deg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c6b2e9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Number of unique entries in Valencia_wind_deg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dc75f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_train['Valencia_wind_deg'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9b37e0",
   "metadata": {},
   "source": [
    "The `Valencia_wind_deg` seems to be a categorical data represented by different levels. This levels, which are represented by integers, can be extracted for the purpose of encoding to train the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00cfa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the digit from the strings of levels\n",
    "df_clean_train['Valencia_wind_deg'] = df_clean_train['Valencia_wind_deg'].str.extract('(\\d+)')\n",
    "\n",
    "# Converting the extracted digits to integers\n",
    "df_clean_train['Valencia_wind_deg'] = pd.to_numeric(df_clean_train['Valencia_wind_deg'])\n",
    "\n",
    "# Checking the output\n",
    "df_clean_train['Valencia_wind_deg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ff3983",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "`Seville_pressure` is another feature in the dataset that is still of type object, which is a string. The same process as above will be repeated on this feature to convert it to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c44c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the digit from the strings of levels\n",
    "df_clean_train['Seville_pressure'] = df_clean_train['Seville_pressure'].str.extract('(\\d+)')\n",
    "\n",
    "# Converting the extracted digits to integers\n",
    "df_clean_train['Seville_pressure'] = pd.to_numeric(df_clean_train['Seville_pressure'])\n",
    "\n",
    "# Checking the output\n",
    "df_clean_train['Seville_pressure']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04f9fea",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Viewing the features and their corresponding datatypes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c2a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b859d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_clean_train.columns, columns=['Features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef5175",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Before moving on to the model training, it will be important to test for multicollinearity, the correlation of each feature with the target variable and check for the significance of each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e9c062",
   "metadata": {},
   "source": [
    "### Evaluating correlation with the output variable\n",
    "\n",
    "It is neccesary to evaluate the correlation of the different features in the train dataset with the target variable to ascertain which features have a very weak relationship with the target variable. This was done using the `corrwith` function of the Pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a4f84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_train.corrwithb(df_clean_train[\"load_shortfall_3h\"]).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2635718e",
   "metadata": {},
   "source": [
    "To test for multicollinearity, it will be more appealing to visualize the correlations between individual features and the target variable. This will be done using `Heatmap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075b648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df_clean_train.corr()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "heatmap = sns.heatmap(correlation_matrix, vmin=-1, vmax=1)\n",
    "#heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992b99d5",
   "metadata": {},
   "source": [
    "### Temperatures\n",
    "\n",
    "From the Exploratory Data Analysis, all the temperature features tend to follow the same trend with time. It is imperative to investigate this features further to test for multicollinearity and their strength of their relationship with the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49556e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all temperature features to compute the correlation with the target variable\n",
    "\n",
    "temperature_list = []\n",
    "\n",
    "for col in df_clean_train.columns:\n",
    "    if (len(col.split('_')) > 1):\n",
    "        if (col.split('_')[1] == 'temp'):\n",
    "            temperature_list.append(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325d4fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b57bf7",
   "metadata": {},
   "source": [
    "## Variables Selection by Correlation and Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8c9b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_titles = [col for col in x if col != 'load_shortfall_3h'] + ['load_shortfall_3h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347e072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "\n",
    "df_pval = df_modified[:len(df_train)].copy()\n",
    "\n",
    "y_name = 'load_shortfall_3h'\n",
    "\n",
    "x_names = [col for col in df_pval.columns if col != y_name]\n",
    "\n",
    "\n",
    "formula_str = y_name + \"~\" + \"+\".join(x_names)\n",
    "print('Formular:\\n\\t{}'.format(formula_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566d1d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols(formula=formula_str, data=df_pval)\n",
    "\n",
    "fitted = model.fit()\n",
    "\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ebe211",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_values = df_pval.corr()['load_shortfall_3h'].sort_values(ascending=False)\n",
    "\n",
    "corr_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dfb4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "dict_corr_p = {}\n",
    "\n",
    "column_titles_pr = [col for col in corr_values.index if col not in ['load_shortfall_3h', 'minute', 'second']]\n",
    "for col in column_titles_pr:\n",
    "    p_val = round(pearsonr(df_pval[col], df_pval['load_shortfall_3h'])[1], 6)\n",
    "    dict_corr_p[col] = {'Correlation coefficient': corr_values[col],\n",
    "                       'P_Value': p_val}\n",
    "    \n",
    "df_corr_p = pd.DataFrame(dict_corr_p).T\n",
    "df_corr_p_sorted = df_corr_p.sort_values('P_Value')\n",
    "df_corr_p_sorted[df_corr_p_sorted['P_Value']<0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856026fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_names_sig = list(df_corr_p.index)\n",
    "\n",
    "x_data = x[x_names_sig]\n",
    "\n",
    "m_col = x_data.corr()\n",
    "\n",
    "r, c = np.where(np.abs(m_col)>0.9)\n",
    "\n",
    "off_diagonal = np.where(r != c)\n",
    "\n",
    "multicol_df = m_col.iloc[r[off_diagonal], c[off_diagonal]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2d523",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 5. Modelling\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Modelling ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, we created two models that enable us to accurately predict the thee hour load shortfall. |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ef2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "y = df_modified_train[['load_shortfall_3h']]\n",
    "x = df_modified_train.drop('load_shortfall_3h',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d6b9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6337d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0932c9",
   "metadata": {},
   "source": [
    "### Building One or More ML Models\n",
    "\n",
    "First we created a base model, and two other models. In this case, our base model is a simple linear regression model. Other models created to improve predictive performance are the random forest and lasso regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706b19c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.34,random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc94f65",
   "metadata": {},
   "source": [
    "#### i. Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a0453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the model object\n",
    "lm = LinearRegression()\n",
    "\n",
    "# fit linear model\n",
    "lm.fit(x_train, y_train)\n",
    "\n",
    "prediction_lm = lm.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f19cb32",
   "metadata": {},
   "source": [
    "#### ii. Random Forest Regression Model\n",
    "Random forests or random decision forests is an ensemble learning method. In the following cells, the random forest regression model will be loaded, trained and used to predict the load shortfall from the given data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d594dbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Random Forest Regression to the dataset\n",
    "\n",
    "# create regressor object\n",
    "Rfr = RandomForestRegressor(n_estimators = 100, max_depth=15, random_state=18)\n",
    "\n",
    "Rfr.fit(x_train, np.ravel(y_train))\n",
    "\n",
    "predictions_Rfr_test = Rfr.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c0477b",
   "metadata": {},
   "source": [
    "#### iii. LASSO Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652cacd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso()\n",
    "\n",
    "lasso.fit(x_train, y_train)\n",
    "\n",
    "predictions_lasso_test = lasso.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b530251",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "## 6. Model Performance\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model performance ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section we compared the relative performance of the various trained ML models on a holdout dataset and indicated which is the best and why. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e66ea47",
   "metadata": {},
   "source": [
    "### The root mean square value (RMSE)\n",
    "\n",
    "The root mean squared error (RMSE) is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the actual values observed. It is a very useful tool in telling how well the model predicted the values using the test dataset. Below is a function that calculates and returns the\n",
    "average RMSE of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9056998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993bae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "# LINEAR REGRESSION MODEL\n",
    "\n",
    "## TRAIN\n",
    "prediction_lm_train = lm.predict(x_train)\n",
    "MSE_lm_train = mean_squared_error(y_train, prediction_lm_train)\n",
    "R_2_lm_train = r2_score(y_train, prediction_lm_train)\n",
    "RMSE_lm_train = np.sqrt(MSE_lm_train)\n",
    "\n",
    "\n",
    "## TEST\n",
    "prediction_lm_test = lm.predict(x_test)\n",
    "MSE_lm_test = mean_squared_error(y_test, prediction_lm_test)\n",
    "R_2_lm_test = r2_score(y_test, prediction_lm_test)\n",
    "RMSE_lm_test = np.sqrt(MSE_lm_test)\n",
    "\n",
    "\n",
    "\n",
    "# REGRESSION MODEL USING RANDOM FOREST\n",
    "## TRAIN\n",
    "prediction_Rfr_train = Rfr.predict(x_train)\n",
    "MSE_Rfr_train = mean_squared_error(y_train, prediction_Rfr_train)\n",
    "R_2_Rfr_train = r2_score(y_train, prediction_Rfr_train)\n",
    "RMSE_Rfr_train = np.sqrt(MSE_Rfr_train)\n",
    "\n",
    "\n",
    "## TEST\n",
    "predictions_Rfr_test = Rfr.predict(x_test)\n",
    "MSE_Rfr_test = mean_squared_error(y_test, predictions_Rfr_test)\n",
    "R_2_Rfr_test = r2_score(y_test, predictions_Rfr_test)\n",
    "RMSE_Rfr_test = np.sqrt(MSE_Rfr_test)\n",
    "\n",
    "\n",
    "# LASSO REGRESSION MODEL\n",
    "## TRAIN\n",
    "prediction_lasso_train = lasso.predict(x_train)\n",
    "MSE_lasso_train = mean_squared_error(y_train, prediction_lasso_train)\n",
    "R_2_lasso_train = r2_score(y_train, prediction_lasso_train)\n",
    "RMSE_lasso_train = np.sqrt(MSE_lasso_train)\n",
    "\n",
    "\n",
    "## TEST\n",
    "predictions_lasso_test = lasso.predict(x_test)\n",
    "MSE_lasso_test = mean_squared_error(y_test, predictions_lasso_test)\n",
    "R_2_lasso_test = r2_score(y_test, predictions_lasso_test)\n",
    "RMSE_lasso_test = np.sqrt(MSE_lasso_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcd47fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of results\n",
    "\n",
    "results_dict = {'Training R-Square Score':\n",
    "                        {\n",
    "                            \"Linear Regression Model\": R_2_lm_train,\n",
    "                           # \"Linear Regression Model p_val\": R_2_lm_corr_pval_train,\n",
    "                            \"LASSO\": R_2_lasso_train,\n",
    "                            \"Random Forest\": R_2_Rfr_train\n",
    "                        },\n",
    "                \n",
    "                    'Test R-Square Score':\n",
    "                        {\n",
    "                            \"Linear Regression Model\": R_2_lm_test,\n",
    "                            #\"Linear Regression Model p_val\": R_2_lm_corr_pval_test,\n",
    "                           \"LASSO\": R_2_lasso_test,\n",
    "                            \"Random Forest\": R_2_Rfr_test\n",
    "                        },\n",
    "                \n",
    "                    'Training RMSE':\n",
    "                        {\n",
    "                            \"Linear Regression Model\": RMSE_lm_train,\n",
    "                         #   \"Linear Regression Model p_val\": RMSE_lm_corr_pval_train,\n",
    "                            \"LASSO\": RMSE_lasso_train,\n",
    "                            \"Random Forest\": RMSE_Rfr_train\n",
    "                        },\n",
    "                \n",
    "                    'Test RMSE':\n",
    "                        {\n",
    "                            \"Linear Regression Model\": RMSE_lm_test,\n",
    "                          #  \"Linear Regression Model p_val\": RMSE_lm_corr_pval_test,\n",
    "                           \"LASSO\": RMSE_lasso_test,\n",
    "                            \"Random Forest\": RMSE_Rfr_test\n",
    "                        }\n",
    "                }\n",
    "\n",
    "# create dataframe from dictionary\n",
    "results_df = pd.DataFrame(data=results_dict)\n",
    "\n",
    "pd.options.display.float_format = \"{:,.5f}\".format\n",
    "results_df = results_df.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd1a1c",
   "metadata": {},
   "source": [
    "### Model Performance Results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1a7b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b55bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axes\n",
    "f, ax = plt.subplots(figsize=(15,5), nrows=1, ncols=3, sharey=True)\n",
    "\n",
    "# Create list of titles and predictions to use in for loop\n",
    "pred = [lm.predict(x_test), Rfr.predict(x_test), lasso.predict(x_test)]\n",
    "title = ['Linear Regression','Random Forest', 'LASSO Regression']\n",
    "\n",
    "# Loop through all axes to plot each model's results \n",
    "for i in range(3):\n",
    "    rmse = round(np.sqrt(mean_squared_error(pred[i],y_test)))\n",
    "    ax[i].set_title(title[i]+\"  (RMSE: \"+str(rmse)+ \")\")\n",
    "    ax[i].set_xlabel('Actual')\n",
    "    ax[i].set_ylabel('Predicted')\n",
    "    ax[i].plot(y_test,y_test,'r')\n",
    "    ax[i].scatter(y_test,pred[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b5c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axes\n",
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "# Plot on axes\n",
    "ax.set_title('Linear Regression')\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.scatter(y_test,prediction_lm)\n",
    "ax.plot(y_test,y_test,'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b82dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axes\n",
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "# Plot on axes\n",
    "ax.set_title('Random Forest')\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.scatter(y_test,predictions_Rfr_test)\n",
    "ax.plot(y_test,y_test,'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db8554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axes\n",
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "# Plot on axes\n",
    "ax.set_title('Lasso Regression')\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.scatter(y_test,predictions_lasso_test)\n",
    "ax.plot(y_test,y_test,'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd10c88b",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 7. Model Explanations\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model explanation ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, we discuss the how the model with the best performance based on the analysis in the previous section works. |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "From our test results as displayed in the previous section, it can be observed that the Random Forest model gives a better RMSE and R_2 score, thus making it the best model among the three prediction models built.\n",
    "\n",
    "### Understanding Random Forest Models\n",
    "\n",
    "A **Random Forest** is a powerful non-parametric algorithm (ie an algorithm that does not make strong assumptions about the form of the mapping function, but instead is free to learn any functional form from the training dataset). It is important to note that non-parametric models like this are good when you have a lot of data with no prior knowledge, and you don’t want to worry too much about choosing just the right features.\n",
    "\n",
    "Random Forest is an example of an ensemble method built on decision trees. Ensemble methods is a machine learning technique that combines several base models in order to produce one optimal predictive model. In other words, a random forest model relies on aggregating the results of an ensemble of decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc89b5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a9ad74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc2149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
